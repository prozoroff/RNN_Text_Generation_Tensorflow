{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN with num_layers LSTM layers and a fully-connected output layer\n",
    "The network allows for a dynamic number of iterations, depending on the inputs it receives.\n",
    "\n",
    "out (fc layer; out_size) <- lstm <- lstm <- in (in_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ModelNetwork:\n",
    "    def __init__(self, in_size, lstm_size, num_layers, out_size, session, learning_rate=0.003, name=\"rnn\"):\n",
    "        self.scope = name\n",
    "\n",
    "        self.in_size = in_size\n",
    "        self.lstm_size = lstm_size\n",
    "        self.num_layers = num_layers\n",
    "        self.out_size = out_size\n",
    "\n",
    "        self.session = session\n",
    "\n",
    "        self.learning_rate = tf.constant( learning_rate )\n",
    "\n",
    "        # Last state of LSTM, used when running the network in TEST mode\n",
    "        self.lstm_last_state = np.zeros((self.num_layers*2*self.lstm_size,))\n",
    "\n",
    "        with tf.variable_scope(self.scope):\n",
    "            ## (batch_size, timesteps, in_size)\n",
    "            self.xinput = tf.placeholder(tf.float32, shape=(None, None, self.in_size), name=\"xinput\")\n",
    "            self.lstm_init_value = tf.placeholder(tf.float32, shape=(None, self.num_layers*2*self.lstm_size), name=\"lstm_init_value\")\n",
    "\n",
    "            # LSTM\n",
    "            self.lstm_cells = [ tf.contrib.rnn.BasicLSTMCell(self.lstm_size, forget_bias=1.0, state_is_tuple=False) for i in range(self.num_layers)]\n",
    "            self.lstm = tf.contrib.rnn.MultiRNNCell(self.lstm_cells, state_is_tuple=False)\n",
    "\n",
    "            # Iteratively compute output of recurrent network\n",
    "            outputs, self.lstm_new_state = tf.nn.dynamic_rnn(self.lstm, self.xinput, initial_state=self.lstm_init_value, dtype=tf.float32)\n",
    "\n",
    "            # Linear activation (FC layer on top of the LSTM net)\n",
    "            self.rnn_out_W = tf.Variable(tf.random_normal( (self.lstm_size, self.out_size), stddev=0.01 ))\n",
    "            self.rnn_out_B = tf.Variable(tf.random_normal( (self.out_size, ), stddev=0.01 ))\n",
    "\n",
    "            outputs_reshaped = tf.reshape( outputs, [-1, self.lstm_size] )\n",
    "            network_output = ( tf.matmul( outputs_reshaped, self.rnn_out_W ) + self.rnn_out_B )\n",
    "\n",
    "            batch_time_shape = tf.shape(outputs)\n",
    "            self.final_outputs = tf.reshape( tf.nn.softmax( network_output), (batch_time_shape[0], batch_time_shape[1], self.out_size) )\n",
    "\n",
    "\n",
    "            ## Training: provide target outputs for supervised training.\n",
    "            self.y_batch = tf.placeholder(tf.float32, (None, None, self.out_size))\n",
    "            y_batch_long = tf.reshape(self.y_batch, [-1, self.out_size])\n",
    "\n",
    "            self.cost = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(logits=network_output, labels=y_batch_long) )\n",
    "            self.train_op = tf.train.RMSPropOptimizer(self.learning_rate, 0.9).minimize(self.cost)\n",
    "\n",
    "\n",
    "    ## Input: X is a single element, not a list!\n",
    "    def run_step(self, x, init_zero_state=True):\n",
    "        ## Reset the initial state of the network.\n",
    "        if init_zero_state:\n",
    "            init_value = np.zeros((self.num_layers*2*self.lstm_size,))\n",
    "        else:\n",
    "            init_value = self.lstm_last_state\n",
    "\n",
    "        out, next_lstm_state = self.session.run([self.final_outputs, self.lstm_new_state], feed_dict={self.xinput:[x], self.lstm_init_value:[init_value]   } )\n",
    "\n",
    "        self.lstm_last_state = next_lstm_state[0]\n",
    "\n",
    "        return out[0][0]\n",
    "\n",
    "\n",
    "    ## xbatch must be (batch_size, timesteps, input_size)\n",
    "    ## ybatch must be (batch_size, timesteps, output_size)\n",
    "    def train_batch(self, xbatch, ybatch):\n",
    "        init_value = np.zeros((xbatch.shape[0], self.num_layers*2*self.lstm_size))\n",
    "\n",
    "        cost, _ = self.session.run([self.cost, self.train_op], feed_dict={self.xinput:xbatch, self.y_batch:ybatch, self.lstm_init_value:init_value   } )\n",
    "\n",
    "        return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embed string to character-arrays -- it generates an array len(data) x len(vocab)\n",
    "\n",
    "Vocab is a list of elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def embed_to_vocab(data_, vocab):\n",
    "    data = np.zeros((len(data_), len(vocab)))\n",
    "\n",
    "    cnt=0\n",
    "    for s in data_:\n",
    "        v = [0.0]*len(vocab)\n",
    "        v[vocab.index(s)] = 1.0\n",
    "        data[cnt, :] = v\n",
    "        cnt += 1\n",
    "\n",
    "    return data\n",
    "\n",
    "def decode_embed(array, vocab):\n",
    "    return vocab[ array.index(1) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage:\n",
      "  C:\\Users\\alex\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py  [ckpt model to load] [prefix, e.g., \"The \"]\n"
     ]
    }
   ],
   "source": [
    "ckpt_file = \"\"\n",
    "TEST_PREFIX = \"The \" # Prefix to prompt the network in test mode\n",
    "\n",
    "print(\"Usage:\")\n",
    "print(' ', sys.argv[0], ' [ckpt model to load] [prefix, e.g., \"The \"]')\n",
    "if len(sys.argv)>=2:\n",
    "    ckpt_file=sys.argv[1]\n",
    "if len(sys.argv)==3:\n",
    "    TEST_PREFIX = sys.argv[2]\n",
    "\n",
    "## Load the data\n",
    "data_ = \"\"\n",
    "with open('data/shakespeare.txt', 'r') as f:\n",
    "    data_ += f.read()\n",
    "data_ = data_.lower()\n",
    "\n",
    "## Convert to 1-hot coding\n",
    "vocab = list(set(data_))\n",
    "\n",
    "data = embed_to_vocab(data_, vocab)\n",
    "\n",
    "\n",
    "in_size = out_size = len(vocab)\n",
    "lstm_size = 256 #128\n",
    "num_layers = 2\n",
    "batch_size = 64 #128\n",
    "time_steps = 100 #50\n",
    "\n",
    "NUM_TRAIN_BATCHES = 20000\n",
    "\n",
    "LEN_TEST_TEXT = 500 # Number of test characters of text to generate after training the network\n",
    "\n",
    "\n",
    "\n",
    "## Initialize the network\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "sess = tf.InteractiveSession(config=config)\n",
    "\n",
    "net = ModelNetwork(in_size = in_size,lstm_size = lstm_size,num_layers = num_layers,out_size = out_size,session = sess,learning_rate = 0.003,name = \"char_rnn_network\")\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "saver = tf.train.Saver(tf.global_variables())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ckpt_file == \"\":\n",
    "    last_time = time.time()\n",
    "\n",
    "    batch = np.zeros((batch_size, time_steps, in_size))\n",
    "    batch_y = np.zeros((batch_size, time_steps, in_size))\n",
    "\n",
    "    possible_batch_ids = range(data.shape[0]-time_steps-1)\n",
    "    for i in range(NUM_TRAIN_BATCHES):\n",
    "        # Sample time_steps consecutive samples from the dataset text file\n",
    "        batch_id = random.sample( possible_batch_ids, batch_size )\n",
    "\n",
    "        for j in range(time_steps):\n",
    "            ind1 = [k+j for k in batch_id]\n",
    "            ind2 = [k+j+1 for k in batch_id]\n",
    "\n",
    "            batch[:, j, :] = data[ind1, :]\n",
    "            batch_y[:, j, :] = data[ind2, :]\n",
    "\n",
    "\n",
    "        cst = net.train_batch(batch, batch_y)\n",
    "\n",
    "        if (i%100) == 0:\n",
    "            new_time = time.time()\n",
    "            diff = new_time - last_time\n",
    "            last_time = new_time\n",
    "\n",
    "            print(\"batch: \",i,\"   loss: \",cst,\"   speed: \",(100.0/diff),\" batches / s\")\n",
    "\n",
    "    saver.save(sess, \"saved/model.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "generate text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from -f\n"
     ]
    },
    {
     "ename": "NotFoundError",
     "evalue": "Unsuccessful TensorSliceReader constructor: Failed to find any matching files for -f\n\t [[Node: save/RestoreV2_17 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_arg_save/Const_0_0, save/RestoreV2_17/tensor_names, save/RestoreV2_17/shape_and_slices)]]\n\nCaused by op 'save/RestoreV2_17', defined at:\n  File \"C:\\Users\\alex\\Anaconda3\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\Users\\alex\\Anaconda3\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\alex\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"C:\\Users\\alex\\Anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"C:\\Users\\alex\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"C:\\Users\\alex\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"C:\\Users\\alex\\Anaconda3\\lib\\site-packages\\tornado\\ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"C:\\Users\\alex\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\alex\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"C:\\Users\\alex\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"C:\\Users\\alex\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"C:\\Users\\alex\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\alex\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"C:\\Users\\alex\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"C:\\Users\\alex\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"C:\\Users\\alex\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\Users\\alex\\Anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\Users\\alex\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\Users\\alex\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"C:\\Users\\alex\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-4-de84e0a96723>\", line 44, in <module>\n    saver = tf.train.Saver(tf.global_variables())\n  File \"C:\\Users\\alex\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1139, in __init__\n    self.build()\n  File \"C:\\Users\\alex\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1170, in build\n    restore_sequentially=self._restore_sequentially)\n  File \"C:\\Users\\alex\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 691, in build\n    restore_sequentially, reshape)\n  File \"C:\\Users\\alex\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 407, in _AddRestoreOps\n    tensors = self.restore_op(filename_tensor, saveable, preferred_shard)\n  File \"C:\\Users\\alex\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 247, in restore_op\n    [spec.tensor.dtype])[0])\n  File \"C:\\Users\\alex\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_io_ops.py\", line 640, in restore_v2\n    dtypes=dtypes, name=name)\n  File \"C:\\Users\\alex\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 767, in apply_op\n    op_def=op_def)\n  File \"C:\\Users\\alex\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2506, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"C:\\Users\\alex\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1269, in __init__\n    self._traceback = _extract_stack()\n\nNotFoundError (see above for traceback): Unsuccessful TensorSliceReader constructor: Failed to find any matching files for -f\n\t [[Node: save/RestoreV2_17 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_arg_save/Const_0_0, save/RestoreV2_17/tensor_names, save/RestoreV2_17/shape_and_slices)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\alex\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1138\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1139\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1140\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\alex\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1120\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1121\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m   1122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\alex\\Anaconda3\\lib\\contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type, value, traceback)\u001b[0m\n\u001b[0;32m     88\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 89\u001b[1;33m                 \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     90\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\alex\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[1;34m()\u001b[0m\n\u001b[0;32m    465\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 466\u001b[1;33m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[0;32m    467\u001b[0m   \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNotFoundError\u001b[0m: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for -f\n\t [[Node: save/RestoreV2_17 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_arg_save/Const_0_0, save/RestoreV2_17/tensor_names, save/RestoreV2_17/shape_and_slices)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-46241c68f73e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mckpt_file\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0msaver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mckpt_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mTEST_PREFIX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTEST_PREFIX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTEST_PREFIX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\alex\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\u001b[0m in \u001b[0;36mrestore\u001b[1;34m(self, sess, save_path)\u001b[0m\n\u001b[0;32m   1546\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Restoring parameters from %s\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1547\u001b[0m     sess.run(self.saver_def.restore_op_name,\n\u001b[1;32m-> 1548\u001b[1;33m              {self.saver_def.filename_tensor_name: save_path})\n\u001b[0m\u001b[0;32m   1549\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1550\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\alex\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    787\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 789\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    790\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\alex\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    995\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 997\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    998\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\alex\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1130\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m-> 1132\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m   1133\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32mC:\\Users\\alex\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1150\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1151\u001b[0m           \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1152\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1154\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNotFoundError\u001b[0m: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for -f\n\t [[Node: save/RestoreV2_17 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_arg_save/Const_0_0, save/RestoreV2_17/tensor_names, save/RestoreV2_17/shape_and_slices)]]\n\nCaused by op 'save/RestoreV2_17', defined at:\n  File \"C:\\Users\\alex\\Anaconda3\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\Users\\alex\\Anaconda3\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\alex\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"C:\\Users\\alex\\Anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"C:\\Users\\alex\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"C:\\Users\\alex\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"C:\\Users\\alex\\Anaconda3\\lib\\site-packages\\tornado\\ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"C:\\Users\\alex\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\alex\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"C:\\Users\\alex\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"C:\\Users\\alex\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"C:\\Users\\alex\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\alex\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"C:\\Users\\alex\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"C:\\Users\\alex\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"C:\\Users\\alex\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\Users\\alex\\Anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\Users\\alex\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\Users\\alex\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"C:\\Users\\alex\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-4-de84e0a96723>\", line 44, in <module>\n    saver = tf.train.Saver(tf.global_variables())\n  File \"C:\\Users\\alex\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1139, in __init__\n    self.build()\n  File \"C:\\Users\\alex\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1170, in build\n    restore_sequentially=self._restore_sequentially)\n  File \"C:\\Users\\alex\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 691, in build\n    restore_sequentially, reshape)\n  File \"C:\\Users\\alex\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 407, in _AddRestoreOps\n    tensors = self.restore_op(filename_tensor, saveable, preferred_shard)\n  File \"C:\\Users\\alex\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 247, in restore_op\n    [spec.tensor.dtype])[0])\n  File \"C:\\Users\\alex\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_io_ops.py\", line 640, in restore_v2\n    dtypes=dtypes, name=name)\n  File \"C:\\Users\\alex\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 767, in apply_op\n    op_def=op_def)\n  File \"C:\\Users\\alex\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2506, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"C:\\Users\\alex\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1269, in __init__\n    self._traceback = _extract_stack()\n\nNotFoundError (see above for traceback): Unsuccessful TensorSliceReader constructor: Failed to find any matching files for -f\n\t [[Node: save/RestoreV2_17 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_arg_save/Const_0_0, save/RestoreV2_17/tensor_names, save/RestoreV2_17/shape_and_slices)]]\n"
     ]
    }
   ],
   "source": [
    "if ckpt_file != \"\":\n",
    "    saver.restore(sess, ckpt_file)\n",
    "\n",
    "TEST_PREFIX = TEST_PREFIX.lower()\n",
    "for i in range(len(TEST_PREFIX)):\n",
    "    out = net.run_step( embed_to_vocab(TEST_PREFIX[i], vocab) , i==0)\n",
    "\n",
    "print(\"SENTENCE:\")\n",
    "gen_str = TEST_PREFIX\n",
    "for i in range(LEN_TEST_TEXT):\n",
    "    element = np.random.choice( range(len(vocab)), p=out ) # Sample character from the network according to the generated output probabilities\n",
    "    gen_str += vocab[element]\n",
    "\n",
    "    out = net.run_step( embed_to_vocab(vocab[element], vocab) , False )\n",
    "print(gen_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
