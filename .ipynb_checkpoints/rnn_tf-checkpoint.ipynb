{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN with num_layers LSTM layers and a fully-connected output layer\n",
    "The network allows for a dynamic number of iterations, depending on the inputs it receives.\n",
    "\n",
    "out (fc layer; out_size) <- lstm <- lstm <- in (in_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ModelNetwork:\n",
    "    def __init__(self, in_size, lstm_size, num_layers, out_size, session, learning_rate=0.003, name=\"rnn\"):\n",
    "        self.scope = name\n",
    "\n",
    "        self.in_size = in_size\n",
    "        self.lstm_size = lstm_size\n",
    "        self.num_layers = num_layers\n",
    "        self.out_size = out_size\n",
    "\n",
    "        self.session = session\n",
    "\n",
    "        self.learning_rate = tf.constant( learning_rate )\n",
    "\n",
    "        # Last state of LSTM, used when running the network in TEST mode\n",
    "        self.lstm_last_state = np.zeros((self.num_layers*2*self.lstm_size,))\n",
    "\n",
    "        with tf.variable_scope(self.scope):\n",
    "            ## (batch_size, timesteps, in_size)\n",
    "            self.xinput = tf.placeholder(tf.float32, shape=(None, None, self.in_size), name=\"xinput\")\n",
    "            self.lstm_init_value = tf.placeholder(tf.float32, shape=(None, self.num_layers*2*self.lstm_size), name=\"lstm_init_value\")\n",
    "\n",
    "            # LSTM\n",
    "            self.lstm_cells = [ tf.contrib.rnn.BasicLSTMCell(self.lstm_size, forget_bias=1.0, state_is_tuple=False) for i in range(self.num_layers)]\n",
    "            self.lstm = tf.contrib.rnn.MultiRNNCell(self.lstm_cells, state_is_tuple=False)\n",
    "\n",
    "            # Iteratively compute output of recurrent network\n",
    "            outputs, self.lstm_new_state = tf.nn.dynamic_rnn(self.lstm, self.xinput, initial_state=self.lstm_init_value, dtype=tf.float32)\n",
    "\n",
    "            # Linear activation (FC layer on top of the LSTM net)\n",
    "            self.rnn_out_W = tf.Variable(tf.random_normal( (self.lstm_size, self.out_size), stddev=0.01 ))\n",
    "            self.rnn_out_B = tf.Variable(tf.random_normal( (self.out_size, ), stddev=0.01 ))\n",
    "\n",
    "            outputs_reshaped = tf.reshape( outputs, [-1, self.lstm_size] )\n",
    "            network_output = ( tf.matmul( outputs_reshaped, self.rnn_out_W ) + self.rnn_out_B )\n",
    "\n",
    "            batch_time_shape = tf.shape(outputs)\n",
    "            self.final_outputs = tf.reshape( tf.nn.softmax( network_output), (batch_time_shape[0], batch_time_shape[1], self.out_size) )\n",
    "\n",
    "\n",
    "            ## Training: provide target outputs for supervised training.\n",
    "            self.y_batch = tf.placeholder(tf.float32, (None, None, self.out_size))\n",
    "            y_batch_long = tf.reshape(self.y_batch, [-1, self.out_size])\n",
    "\n",
    "            self.cost = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(logits=network_output, labels=y_batch_long) )\n",
    "            self.train_op = tf.train.RMSPropOptimizer(self.learning_rate, 0.9).minimize(self.cost)\n",
    "\n",
    "\n",
    "    ## Input: X is a single element, not a list!\n",
    "    def run_step(self, x, init_zero_state=True):\n",
    "        ## Reset the initial state of the network.\n",
    "        if init_zero_state:\n",
    "            init_value = np.zeros((self.num_layers*2*self.lstm_size,))\n",
    "        else:\n",
    "            init_value = self.lstm_last_state\n",
    "\n",
    "        out, next_lstm_state = self.session.run([self.final_outputs, self.lstm_new_state], feed_dict={self.xinput:[x], self.lstm_init_value:[init_value]   } )\n",
    "\n",
    "        self.lstm_last_state = next_lstm_state[0]\n",
    "\n",
    "        return out[0][0]\n",
    "\n",
    "\n",
    "    ## xbatch must be (batch_size, timesteps, input_size)\n",
    "    ## ybatch must be (batch_size, timesteps, output_size)\n",
    "    def train_batch(self, xbatch, ybatch):\n",
    "        init_value = np.zeros((xbatch.shape[0], self.num_layers*2*self.lstm_size))\n",
    "\n",
    "        cost, _ = self.session.run([self.cost, self.train_op], feed_dict={self.xinput:xbatch, self.y_batch:ybatch, self.lstm_init_value:init_value   } )\n",
    "\n",
    "        return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embed string to character-arrays -- it generates an array len(data) x len(vocab)\n",
    "\n",
    "Vocab is a list of elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def embed_to_vocab(data_, vocab):\n",
    "    data = np.zeros((len(data_), len(vocab)))\n",
    "\n",
    "    cnt=0\n",
    "    for s in data_:\n",
    "        v = [0.0]*len(vocab)\n",
    "        v[vocab.index(s)] = 1.0\n",
    "        data[cnt, :] = v\n",
    "        cnt += 1\n",
    "\n",
    "    return data\n",
    "\n",
    "def decode_embed(array, vocab):\n",
    "    return vocab[ array.index(1) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000020D78199860>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n"
     ]
    }
   ],
   "source": [
    "ckpt_file = \"\"\n",
    "TEST_PREFIX = \"The \" # Prefix to prompt the network in test mode\n",
    "\n",
    "## Load the data\n",
    "data_ = \"\"\n",
    "with open('data/dostoevski.txt', 'r', encoding=\"utf-8\") as f:\n",
    "    data_ += f.read()\n",
    "data_ = data_.lower()\n",
    "\n",
    "## Convert to 1-hot coding\n",
    "vocab = list(set(data_))\n",
    "\n",
    "data = embed_to_vocab(data_, vocab)\n",
    "\n",
    "\n",
    "in_size = out_size = len(vocab)\n",
    "lstm_size = 512\n",
    "num_layers = 2\n",
    "batch_size = 64 #128\n",
    "time_steps = 100 #50\n",
    "\n",
    "NUM_TRAIN_BATCHES = 2500\n",
    "\n",
    "LEN_TEST_TEXT = 500 # Number of test characters of text to generate after training the network\n",
    "\n",
    "\n",
    "\n",
    "## Initialize the network\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "sess = tf.InteractiveSession(config=config)\n",
    "\n",
    "net = ModelNetwork(in_size = in_size,lstm_size = lstm_size,num_layers = num_layers,out_size = out_size,session = sess,learning_rate = 0.003)\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "saver = tf.train.Saver(tf.global_variables())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch:  0    loss:  4.55155    speed:  273.4786651244127  batches / s\n",
      "batch:  10    loss:  4.54952    speed:  32.496096528569815  batches / s\n",
      "batch:  20    loss:  4.54618    speed:  32.026432949191616  batches / s\n",
      "batch:  30    loss:  4.5405    speed:  32.18142316593761  batches / s\n",
      "batch:  40    loss:  4.53151    speed:  33.34346125196656  batches / s\n",
      "batch:  50    loss:  4.51881    speed:  33.8641754214771  batches / s\n",
      "batch:  60    loss:  4.50038    speed:  33.92192586226739  batches / s\n",
      "batch:  70    loss:  4.4721    speed:  32.80567013591075  batches / s\n",
      "batch:  80    loss:  4.40019    speed:  32.37977409757958  batches / s\n",
      "batch:  90    loss:  3.55288    speed:  32.17070807781372  batches / s\n",
      "batch:  100    loss:  3.31405    speed:  30.809267143703135  batches / s\n",
      "batch:  110    loss:  3.25453    speed:  32.47462223308244  batches / s\n",
      "batch:  120    loss:  3.26619    speed:  31.326625260858904  batches / s\n",
      "batch:  130    loss:  3.24238    speed:  28.22428294051218  batches / s\n",
      "batch:  140    loss:  3.26222    speed:  30.781160271697072  batches / s\n",
      "batch:  150    loss:  3.21318    speed:  32.83820963148319  batches / s\n",
      "batch:  160    loss:  3.21787    speed:  33.466364592248205  batches / s\n",
      "batch:  170    loss:  3.23366    speed:  33.088897680749994  batches / s\n",
      "batch:  180    loss:  3.1848    speed:  33.56768989480085  batches / s\n",
      "batch:  190    loss:  3.18443    speed:  32.9359553604879  batches / s\n",
      "batch:  200    loss:  3.05504    speed:  32.295991560223  batches / s\n",
      "batch:  210    loss:  2.97395    speed:  32.82199731401864  batches / s\n",
      "batch:  220    loss:  2.85865    speed:  30.456417404210654  batches / s\n",
      "batch:  230    loss:  2.8062    speed:  32.75184157191198  batches / s\n",
      "batch:  240    loss:  2.72797    speed:  33.72680023248416  batches / s\n",
      "batch:  250    loss:  2.66655    speed:  33.715362166694064  batches / s\n",
      "batch:  260    loss:  2.62684    speed:  33.07781769418265  batches / s\n",
      "batch:  270    loss:  2.57229    speed:  32.903209054143666  batches / s\n",
      "batch:  280    loss:  2.62541    speed:  33.29888305190645  batches / s\n",
      "batch:  290    loss:  2.52407    speed:  32.680892655100465  batches / s\n",
      "batch:  300    loss:  2.4709    speed:  33.5563379999984  batches / s\n",
      "batch:  310    loss:  2.49948    speed:  32.32712120566336  batches / s\n",
      "batch:  320    loss:  2.47609    speed:  32.979392754526394  batches / s\n",
      "batch:  330    loss:  2.40794    speed:  32.49561565663812  batches / s\n",
      "batch:  340    loss:  2.41185    speed:  31.470241018763577  batches / s\n",
      "batch:  350    loss:  2.38001    speed:  33.034097396898595  batches / s\n",
      "batch:  360    loss:  2.34717    speed:  33.99652278226051  batches / s\n",
      "batch:  370    loss:  2.38888    speed:  33.77800183856912  batches / s\n",
      "batch:  380    loss:  2.35512    speed:  33.7667030017502  batches / s\n",
      "batch:  390    loss:  2.32564    speed:  33.47187429149751  batches / s\n",
      "batch:  400    loss:  2.29239    speed:  33.17700245756218  batches / s\n",
      "batch:  410    loss:  2.27577    speed:  33.50011625151444  batches / s\n",
      "batch:  420    loss:  2.30301    speed:  32.233322213105225  batches / s\n",
      "batch:  430    loss:  2.23289    speed:  33.26547477987212  batches / s\n",
      "batch:  440    loss:  2.21703    speed:  32.75737688504298  batches / s\n",
      "batch:  450    loss:  2.29806    speed:  33.23762689450251  batches / s\n",
      "batch:  460    loss:  2.17829    speed:  32.719352968254775  batches / s\n",
      "batch:  470    loss:  2.1992    speed:  31.19453312665416  batches / s\n",
      "batch:  480    loss:  2.19818    speed:  32.67164162944117  batches / s\n",
      "batch:  490    loss:  2.16717    speed:  31.766094885582564  batches / s\n",
      "batch:  500    loss:  2.13497    speed:  33.47042391361075  batches / s\n",
      "batch:  510    loss:  2.112    speed:  34.604421504107385  batches / s\n",
      "batch:  520    loss:  2.12753    speed:  33.25373575946971  batches / s\n",
      "batch:  530    loss:  2.07707    speed:  34.62260586638112  batches / s\n",
      "batch:  540    loss:  2.16371    speed:  33.19902186338287  batches / s\n",
      "batch:  550    loss:  2.09986    speed:  33.06696671279203  batches / s\n",
      "batch:  560    loss:  2.0538    speed:  33.76107136383233  batches / s\n",
      "batch:  570    loss:  2.02055    speed:  33.23230989093477  batches / s\n",
      "batch:  580    loss:  2.04897    speed:  33.87570101828921  batches / s\n",
      "batch:  590    loss:  2.00467    speed:  34.083932711459056  batches / s\n",
      "batch:  600    loss:  2.00108    speed:  33.143526512741396  batches / s\n",
      "batch:  610    loss:  2.00459    speed:  32.543470393950734  batches / s\n",
      "batch:  620    loss:  1.98774    speed:  32.021188336673276  batches / s\n",
      "batch:  630    loss:  1.96232    speed:  32.08787738392436  batches / s\n",
      "batch:  640    loss:  1.94343    speed:  33.78395750456944  batches / s\n",
      "batch:  650    loss:  1.99414    speed:  32.85967592867883  batches / s\n",
      "batch:  660    loss:  1.9471    speed:  33.33203267828266  batches / s\n",
      "batch:  670    loss:  1.91503    speed:  33.18799669093604  batches / s\n",
      "batch:  680    loss:  1.98514    speed:  33.21013850886201  batches / s\n",
      "batch:  690    loss:  1.94487    speed:  33.01217402939319  batches / s\n",
      "batch:  700    loss:  1.90834    speed:  32.946857679655174  batches / s\n",
      "batch:  710    loss:  1.91041    speed:  33.34324654621517  batches / s\n",
      "batch:  720    loss:  1.89254    speed:  33.29893328080835  batches / s\n",
      "batch:  730    loss:  1.86229    speed:  34.318788289288214  batches / s\n",
      "batch:  740    loss:  1.91887    speed:  31.30249775677961  batches / s\n",
      "batch:  750    loss:  1.82627    speed:  33.86423283856167  batches / s\n",
      "batch:  760    loss:  1.82775    speed:  34.16585874150049  batches / s\n",
      "batch:  770    loss:  1.81815    speed:  33.738145649743636  batches / s\n",
      "batch:  780    loss:  1.80854    speed:  33.09972649328213  batches / s\n",
      "batch:  790    loss:  1.79159    speed:  33.567937052434935  batches / s\n",
      "batch:  800    loss:  1.78672    speed:  34.1191697684742  batches / s\n",
      "batch:  810    loss:  1.81658    speed:  33.8068338213099  batches / s\n",
      "batch:  820    loss:  1.81779    speed:  34.29504305743742  batches / s\n",
      "batch:  830    loss:  1.77722    speed:  34.04906411638123  batches / s\n",
      "batch:  840    loss:  1.75984    speed:  33.455307937218976  batches / s\n",
      "batch:  850    loss:  1.79154    speed:  33.066961498939555  batches / s\n",
      "batch:  860    loss:  1.80045    speed:  33.52244890195163  batches / s\n",
      "batch:  870    loss:  1.79919    speed:  33.51143816529146  batches / s\n",
      "batch:  880    loss:  1.76791    speed:  33.573190012723906  batches / s\n",
      "batch:  890    loss:  1.75329    speed:  33.858423763241994  batches / s\n",
      "batch:  900    loss:  1.74701    speed:  33.09964813062886  batches / s\n",
      "batch:  910    loss:  1.79276    speed:  32.849276295333844  batches / s\n",
      "batch:  920    loss:  1.74794    speed:  32.55911041357243  batches / s\n",
      "batch:  930    loss:  1.72208    speed:  33.743997680409095  batches / s\n",
      "batch:  940    loss:  1.76122    speed:  33.778151452701536  batches / s\n",
      "batch:  950    loss:  1.74925    speed:  32.75209476398024  batches / s\n",
      "batch:  960    loss:  1.76314    speed:  34.08392163249014  batches / s\n",
      "batch:  970    loss:  1.72519    speed:  32.591696744198046  batches / s\n",
      "batch:  980    loss:  1.71    speed:  33.48884209342949  batches / s\n",
      "batch:  990    loss:  1.67631    speed:  32.66599023736189  batches / s\n",
      "batch:  1000    loss:  1.69993    speed:  31.94383167353973  batches / s\n",
      "batch:  1010    loss:  1.70411    speed:  33.52283203760807  batches / s\n",
      "batch:  1020    loss:  1.67015    speed:  32.40596461407711  batches / s\n",
      "batch:  1030    loss:  1.69152    speed:  32.2279402905806  batches / s\n",
      "batch:  1040    loss:  1.67049    speed:  29.066880094781375  batches / s\n",
      "batch:  1050    loss:  1.67096    speed:  30.795509144883763  batches / s\n",
      "batch:  1060    loss:  1.72299    speed:  34.307023809017466  batches / s\n",
      "batch:  1070    loss:  1.71033    speed:  31.872943018135256  batches / s\n",
      "batch:  1080    loss:  1.73671    speed:  33.554163566691464  batches / s\n",
      "batch:  1090    loss:  1.60435    speed:  33.44572803257764  batches / s\n",
      "batch:  1100    loss:  1.65355    speed:  34.20107584513303  batches / s\n",
      "batch:  1110    loss:  1.63505    speed:  29.356285849308044  batches / s\n",
      "batch:  1120    loss:  1.68782    speed:  33.455281252058896  batches / s\n",
      "batch:  1130    loss:  1.68307    speed:  31.836363539758793  batches / s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch:  1140    loss:  1.60936    speed:  31.27299916149104  batches / s\n",
      "batch:  1150    loss:  1.63791    speed:  33.55649102628937  batches / s\n",
      "batch:  1160    loss:  1.63223    speed:  33.443941244673496  batches / s\n",
      "batch:  1170    loss:  1.56051    speed:  33.97972495592637  batches / s\n",
      "batch:  1180    loss:  1.62286    speed:  33.287656729928734  batches / s\n",
      "batch:  1190    loss:  1.6384    speed:  33.749710525074924  batches / s\n",
      "batch:  1200    loss:  1.62572    speed:  33.92170364220227  batches / s\n",
      "batch:  1210    loss:  1.56747    speed:  32.485306769496816  batches / s\n",
      "batch:  1220    loss:  1.60776    speed:  31.21883831763945  batches / s\n",
      "batch:  1230    loss:  1.61258    speed:  32.259412716655596  batches / s\n",
      "batch:  1240    loss:  1.61162    speed:  32.015742604147626  batches / s\n",
      "batch:  1250    loss:  1.57572    speed:  32.94674898305691  batches / s\n",
      "batch:  1260    loss:  1.594    speed:  32.82734825515411  batches / s\n",
      "batch:  1270    loss:  1.61038    speed:  32.21873352068621  batches / s\n",
      "batch:  1280    loss:  1.62538    speed:  29.246163305600533  batches / s\n",
      "batch:  1290    loss:  1.57927    speed:  30.88527467529919  batches / s\n",
      "batch:  1300    loss:  1.60897    speed:  33.46867721648882  batches / s\n",
      "batch:  1310    loss:  1.60382    speed:  33.13384685230335  batches / s\n",
      "batch:  1320    loss:  1.53476    speed:  32.21233965749721  batches / s\n",
      "batch:  1330    loss:  1.57519    speed:  31.549708024628952  batches / s\n",
      "batch:  1340    loss:  1.60378    speed:  33.08887940805966  batches / s\n",
      "batch:  1350    loss:  1.58548    speed:  31.647204367571103  batches / s\n",
      "batch:  1360    loss:  1.61109    speed:  29.57169901632932  batches / s\n",
      "batch:  1370    loss:  1.59421    speed:  30.45261621579393  batches / s\n",
      "batch:  1380    loss:  1.63065    speed:  33.65834473453203  batches / s\n",
      "batch:  1390    loss:  1.56157    speed:  33.57228439795146  batches / s\n",
      "batch:  1400    loss:  1.55708    speed:  33.92177222833149  batches / s\n",
      "batch:  1410    loss:  1.57406    speed:  33.52278113096739  batches / s\n",
      "batch:  1420    loss:  1.49    speed:  32.689387131665086  batches / s\n",
      "batch:  1430    loss:  1.57638    speed:  33.51805552470015  batches / s\n",
      "batch:  1440    loss:  1.56958    speed:  33.87557242660421  batches / s\n",
      "batch:  1450    loss:  1.57724    speed:  34.18941699857098  batches / s\n",
      "batch:  1460    loss:  1.59463    speed:  33.99138169409066  batches / s\n",
      "batch:  1470    loss:  1.48149    speed:  33.86431486330595  batches / s\n",
      "batch:  1480    loss:  1.61079    speed:  33.72672972051944  batches / s\n",
      "batch:  1490    loss:  1.54636    speed:  33.41209114578437  batches / s\n",
      "batch:  1500    loss:  1.5041    speed:  33.61289154096805  batches / s\n",
      "batch:  1510    loss:  1.53686    speed:  33.09449268336484  batches / s\n",
      "batch:  1520    loss:  1.51446    speed:  32.67890657790242  batches / s\n",
      "batch:  1530    loss:  1.50265    speed:  30.665041746836096  batches / s\n",
      "batch:  1540    loss:  1.54254    speed:  33.34243546055214  batches / s\n",
      "batch:  1550    loss:  1.51989    speed:  32.63821464941869  batches / s\n",
      "batch:  1560    loss:  1.51995    speed:  32.89250584379984  batches / s\n",
      "batch:  1570    loss:  1.54394    speed:  30.780380947488542  batches / s\n",
      "batch:  1580    loss:  1.55321    speed:  29.84870139449658  batches / s\n",
      "batch:  1590    loss:  1.46888    speed:  28.97968301122546  batches / s\n",
      "batch:  1600    loss:  1.47787    speed:  26.842107721973548  batches / s\n",
      "batch:  1610    loss:  1.54303    speed:  27.62979131994692  batches / s\n",
      "batch:  1620    loss:  1.50928    speed:  30.386861771824105  batches / s\n",
      "batch:  1630    loss:  1.49652    speed:  33.669949556534526  batches / s\n",
      "batch:  1640    loss:  1.49562    speed:  33.15499651636911  batches / s\n",
      "batch:  1650    loss:  1.48017    speed:  33.12181342130641  batches / s\n",
      "batch:  1660    loss:  1.4862    speed:  33.898794942349554  batches / s\n",
      "batch:  1670    loss:  1.51929    speed:  30.715667861005052  batches / s\n",
      "batch:  1680    loss:  1.49633    speed:  29.329213835448883  batches / s\n",
      "batch:  1690    loss:  1.48713    speed:  31.00217886502405  batches / s\n",
      "batch:  1700    loss:  1.49063    speed:  32.94813621252254  batches / s\n",
      "batch:  1710    loss:  1.52576    speed:  31.93531899160232  batches / s\n",
      "batch:  1720    loss:  1.51422    speed:  29.439794438935316  batches / s\n",
      "batch:  1730    loss:  1.49584    speed:  32.51705723199735  batches / s\n",
      "batch:  1740    loss:  1.4879    speed:  30.926931829049433  batches / s\n",
      "batch:  1750    loss:  1.501    speed:  30.930407571132385  batches / s\n",
      "batch:  1760    loss:  1.46869    speed:  33.75539270514653  batches / s\n",
      "batch:  1770    loss:  1.48881    speed:  33.635765972983954  batches / s\n",
      "batch:  1780    loss:  1.47621    speed:  30.694509169338275  batches / s\n",
      "batch:  1790    loss:  1.52179    speed:  33.15495982485585  batches / s\n",
      "batch:  1800    loss:  1.41177    speed:  33.46651946933639  batches / s\n",
      "batch:  1810    loss:  1.50379    speed:  31.599956965695814  batches / s\n",
      "batch:  1820    loss:  1.45608    speed:  32.30092359775712  batches / s\n",
      "batch:  1830    loss:  1.47641    speed:  32.696667621614566  batches / s\n",
      "batch:  1840    loss:  1.48071    speed:  33.02319707514729  batches / s\n",
      "batch:  1850    loss:  1.50211    speed:  32.282397085959005  batches / s\n",
      "batch:  1860    loss:  1.43589    speed:  33.48891161420504  batches / s\n",
      "batch:  1870    loss:  1.48464    speed:  34.24808448359036  batches / s\n",
      "batch:  1880    loss:  1.47186    speed:  34.73705647725024  batches / s\n",
      "batch:  1890    loss:  1.4895    speed:  34.63462499670731  batches / s\n",
      "batch:  1900    loss:  1.43266    speed:  34.49096943190073  batches / s\n",
      "batch:  1910    loss:  1.40093    speed:  32.26457433426061  batches / s\n",
      "batch:  1920    loss:  1.44235    speed:  33.97973596726062  batches / s\n",
      "batch:  1930    loss:  1.4629    speed:  34.30688069751287  batches / s\n",
      "batch:  1940    loss:  1.47643    speed:  34.59265441853253  batches / s\n",
      "batch:  1950    loss:  1.47564    speed:  34.27140066884036  batches / s\n",
      "batch:  1960    loss:  1.42996    speed:  34.52683188240325  batches / s\n",
      "batch:  1970    loss:  1.49554    speed:  34.598575489997536  batches / s\n",
      "batch:  1980    loss:  1.45215    speed:  34.42541143346413  batches / s\n",
      "batch:  1990    loss:  1.42947    speed:  34.12484655508851  batches / s\n",
      "batch:  2000    loss:  1.48706    speed:  34.562518355371864  batches / s\n",
      "batch:  2010    loss:  1.46871    speed:  34.652534788821576  batches / s\n",
      "batch:  2020    loss:  1.4455    speed:  32.91802906088704  batches / s\n",
      "batch:  2030    loss:  1.42241    speed:  33.132776336475686  batches / s\n",
      "batch:  2040    loss:  1.42372    speed:  31.89430048945154  batches / s\n",
      "batch:  2050    loss:  1.41703    speed:  33.53394410151174  batches / s\n",
      "batch:  2060    loss:  1.50973    speed:  32.57789019468034  batches / s\n",
      "batch:  2070    loss:  1.45901    speed:  34.165911619991164  batches / s\n",
      "batch:  2080    loss:  1.38706    speed:  33.89863055902501  batches / s\n",
      "batch:  2090    loss:  1.41732    speed:  34.20117345391974  batches / s\n",
      "batch:  2100    loss:  1.45978    speed:  33.01497522836261  batches / s\n",
      "batch:  2110    loss:  1.44707    speed:  29.43810217313108  batches / s\n",
      "batch:  2120    loss:  1.44639    speed:  31.27819512987415  batches / s\n",
      "batch:  2130    loss:  1.47286    speed:  32.17101652123517  batches / s\n",
      "batch:  2140    loss:  1.37492    speed:  34.077735153454064  batches / s\n",
      "batch:  2150    loss:  1.45201    speed:  34.461106468063264  batches / s\n",
      "batch:  2160    loss:  1.49836    speed:  33.711286571060754  batches / s\n",
      "batch:  2170    loss:  1.4288    speed:  33.447298963527864  batches / s\n",
      "batch:  2180    loss:  1.43567    speed:  33.806801122652644  batches / s\n",
      "batch:  2190    loss:  1.42527    speed:  29.95523181554971  batches / s\n",
      "batch:  2200    loss:  1.48171    speed:  31.283150165391888  batches / s\n",
      "batch:  2210    loss:  1.44304    speed:  31.896063783245378  batches / s\n",
      "batch:  2220    loss:  1.43631    speed:  34.52062280763843  batches / s\n",
      "batch:  2230    loss:  1.40271    speed:  31.84812899729022  batches / s\n",
      "batch:  2240    loss:  1.38862    speed:  33.62414965686453  batches / s\n",
      "batch:  2250    loss:  1.45381    speed:  29.56795703045307  batches / s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch:  2260    loss:  1.4087    speed:  30.673061080527624  batches / s\n",
      "batch:  2270    loss:  1.48642    speed:  32.104743587496344  batches / s\n",
      "batch:  2280    loss:  1.41815    speed:  32.55955525349627  batches / s\n",
      "batch:  2290    loss:  1.4519    speed:  32.86127982352406  batches / s\n",
      "batch:  2300    loss:  1.38953    speed:  32.21245098402135  batches / s\n",
      "batch:  2310    loss:  1.42329    speed:  30.470600072574925  batches / s\n",
      "batch:  2320    loss:  1.44504    speed:  32.559519868057336  batches / s\n",
      "batch:  2330    loss:  1.4476    speed:  33.18785225908703  batches / s\n",
      "batch:  2340    loss:  1.38164    speed:  32.69301805226871  batches / s\n",
      "batch:  2350    loss:  1.38456    speed:  33.160550975512855  batches / s\n",
      "batch:  2360    loss:  1.41949    speed:  32.650122853336065  batches / s\n",
      "batch:  2370    loss:  1.37504    speed:  33.51405694531684  batches / s\n",
      "batch:  2380    loss:  1.43757    speed:  31.322079808610123  batches / s\n",
      "batch:  2390    loss:  1.42388    speed:  31.782886936369195  batches / s\n",
      "batch:  2400    loss:  1.42493    speed:  33.82973022543937  batches / s\n",
      "batch:  2410    loss:  1.40678    speed:  32.85239472212915  batches / s\n",
      "batch:  2420    loss:  1.44303    speed:  34.22452632168168  batches / s\n",
      "batch:  2430    loss:  1.42289    speed:  34.10740027337613  batches / s\n",
      "batch:  2440    loss:  1.38336    speed:  33.15505417462546  batches / s\n",
      "batch:  2450    loss:  1.40372    speed:  31.35107587912876  batches / s\n",
      "batch:  2460    loss:  1.46053    speed:  31.56297493416203  batches / s\n",
      "batch:  2470    loss:  1.38609    speed:  32.53109988953103  batches / s\n",
      "batch:  2480    loss:  1.37155    speed:  32.55965635517418  batches / s\n",
      "batch:  2490    loss:  1.38209    speed:  33.13243870646874  batches / s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'./model.ckpt'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if ckpt_file == \"\":\n",
    "    last_time = time.time()\n",
    "\n",
    "    batch = np.zeros((batch_size, time_steps, in_size))\n",
    "    batch_y = np.zeros((batch_size, time_steps, in_size))\n",
    "\n",
    "    possible_batch_ids = range(data.shape[0]-time_steps-1)\n",
    "    for i in range(NUM_TRAIN_BATCHES):\n",
    "        # Sample time_steps consecutive samples from the dataset text file\n",
    "        batch_id = random.sample( possible_batch_ids, batch_size )\n",
    "\n",
    "        for j in range(time_steps):\n",
    "            ind1 = [k+j for k in batch_id]\n",
    "            ind2 = [k+j+1 for k in batch_id]\n",
    "\n",
    "            batch[:, j, :] = data[ind1, :]\n",
    "            batch_y[:, j, :] = data[ind2, :]\n",
    "\n",
    "\n",
    "        cst = net.train_batch(batch, batch_y)\n",
    "\n",
    "        if (i%10) == 0:\n",
    "            new_time = time.time()\n",
    "            diff = new_time - last_time\n",
    "            last_time = new_time\n",
    "\n",
    "            print(\"batch: \",i,\"   loss: \",cst,\"   speed: \",(100.0/diff),\" batches / s\")\n",
    "\n",
    "saver.save(sess, \"./model.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "generate text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SENTENCE:\n",
      "город просыпался вскорить смотреть в грядь. а в этом чутом что-нибудь всё промельно не совсем подамил, спысьми!.. и смешно, наяне в эту оди рукой и — «случайно бывал, убежал.\n",
      "\n",
      "— заясть! — кричко услышал сположиться у многого я потой дал предореги, то скументе и ундерживается подлятон изумительности, и известиям, и как она часта своему) доеже в пьяночеки, из бешит: лучше как уж я за беспокоил, а самаясь надобным всеми назад то, разумеется, руки женстое. мечтается. пожалуй» (сейчас с эти и, и обидни начилось еще в\n"
     ]
    }
   ],
   "source": [
    "TEST_PREFIX = \"город просыпался \"\n",
    "\n",
    "for i in range(len(TEST_PREFIX)):\n",
    "    out = net.run_step( embed_to_vocab(TEST_PREFIX[i], vocab) , i==0)\n",
    "\n",
    "print(\"SENTENCE:\")\n",
    "gen_str = TEST_PREFIX\n",
    "for i in range(LEN_TEST_TEXT):\n",
    "    element = np.random.choice( range(len(vocab)), p=out ) # Sample character from the network according to the generated output probabilities\n",
    "    gen_str += vocab[element]\n",
    "\n",
    "    out = net.run_step( embed_to_vocab(vocab[element], vocab) , False )\n",
    "print(gen_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
