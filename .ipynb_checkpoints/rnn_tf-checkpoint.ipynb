{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN with num_layers LSTM layers and a fully-connected output layer\n",
    "The network allows for a dynamic number of iterations, depending on the inputs it receives.\n",
    "\n",
    "out (fc layer; out_size) <- lstm <- lstm <- in (in_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ModelNetwork:\n",
    "    def __init__(self, in_size, lstm_size, num_layers, out_size, session, learning_rate=0.003, name=\"rnn\"):\n",
    "        self.scope = name\n",
    "\n",
    "        self.in_size = in_size\n",
    "        self.lstm_size = lstm_size\n",
    "        self.num_layers = num_layers\n",
    "        self.out_size = out_size\n",
    "\n",
    "        self.session = session\n",
    "\n",
    "        self.learning_rate = tf.constant( learning_rate )\n",
    "\n",
    "        # Last state of LSTM, used when running the network in TEST mode\n",
    "        self.lstm_last_state = np.zeros((self.num_layers*2*self.lstm_size,))\n",
    "\n",
    "        with tf.variable_scope(self.scope):\n",
    "            ## (batch_size, timesteps, in_size)\n",
    "            self.xinput = tf.placeholder(tf.float32, shape=(None, None, self.in_size), name=\"xinput\")\n",
    "            self.lstm_init_value = tf.placeholder(tf.float32, shape=(None, self.num_layers*2*self.lstm_size), name=\"lstm_init_value\")\n",
    "\n",
    "            # LSTM\n",
    "            self.lstm_cells = [ tf.contrib.rnn.BasicLSTMCell(self.lstm_size, forget_bias=1.0, state_is_tuple=False) for i in range(self.num_layers)]\n",
    "            self.lstm = tf.contrib.rnn.MultiRNNCell(self.lstm_cells, state_is_tuple=False)\n",
    "\n",
    "            # Iteratively compute output of recurrent network\n",
    "            outputs, self.lstm_new_state = tf.nn.dynamic_rnn(self.lstm, self.xinput, initial_state=self.lstm_init_value, dtype=tf.float32)\n",
    "\n",
    "            # Linear activation (FC layer on top of the LSTM net)\n",
    "            self.rnn_out_W = tf.Variable(tf.random_normal( (self.lstm_size, self.out_size), stddev=0.01 ))\n",
    "            self.rnn_out_B = tf.Variable(tf.random_normal( (self.out_size, ), stddev=0.01 ))\n",
    "\n",
    "            outputs_reshaped = tf.reshape( outputs, [-1, self.lstm_size] )\n",
    "            network_output = ( tf.matmul( outputs_reshaped, self.rnn_out_W ) + self.rnn_out_B )\n",
    "\n",
    "            batch_time_shape = tf.shape(outputs)\n",
    "            self.final_outputs = tf.reshape( tf.nn.softmax( network_output), (batch_time_shape[0], batch_time_shape[1], self.out_size) )\n",
    "\n",
    "\n",
    "            ## Training: provide target outputs for supervised training.\n",
    "            self.y_batch = tf.placeholder(tf.float32, (None, None, self.out_size))\n",
    "            y_batch_long = tf.reshape(self.y_batch, [-1, self.out_size])\n",
    "\n",
    "            self.cost = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(logits=network_output, labels=y_batch_long) )\n",
    "            self.train_op = tf.train.RMSPropOptimizer(self.learning_rate, 0.9).minimize(self.cost)\n",
    "\n",
    "\n",
    "    ## Input: X is a single element, not a list!\n",
    "    def run_step(self, x, init_zero_state=True):\n",
    "        ## Reset the initial state of the network.\n",
    "        if init_zero_state:\n",
    "            init_value = np.zeros((self.num_layers*2*self.lstm_size,))\n",
    "        else:\n",
    "            init_value = self.lstm_last_state\n",
    "\n",
    "        out, next_lstm_state = self.session.run([self.final_outputs, self.lstm_new_state], feed_dict={self.xinput:[x], self.lstm_init_value:[init_value]   } )\n",
    "\n",
    "        self.lstm_last_state = next_lstm_state[0]\n",
    "\n",
    "        return out[0][0]\n",
    "\n",
    "\n",
    "    ## xbatch must be (batch_size, timesteps, input_size)\n",
    "    ## ybatch must be (batch_size, timesteps, output_size)\n",
    "    def train_batch(self, xbatch, ybatch):\n",
    "        init_value = np.zeros((xbatch.shape[0], self.num_layers*2*self.lstm_size))\n",
    "\n",
    "        cost, _ = self.session.run([self.cost, self.train_op], feed_dict={self.xinput:xbatch, self.y_batch:ybatch, self.lstm_init_value:init_value   } )\n",
    "\n",
    "        return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embed string to character-arrays -- it generates an array len(data) x len(vocab)\n",
    "\n",
    "Vocab is a list of elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def embed_to_vocab(data_, vocab):\n",
    "    data = np.zeros((len(data_), len(vocab)))\n",
    "\n",
    "    cnt=0\n",
    "    for s in data_:\n",
    "        v = [0.0]*len(vocab)\n",
    "        v[vocab.index(s)] = 1.0\n",
    "        data[cnt, :] = v\n",
    "        cnt += 1\n",
    "\n",
    "    return data\n",
    "\n",
    "def decode_embed(array, vocab):\n",
    "    return vocab[ array.index(1) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x000002779F203FD0>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n",
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000277D58900B8>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n",
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000277D58902B0>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n"
     ]
    }
   ],
   "source": [
    "ckpt_file = \"\"\n",
    "TEST_PREFIX = \"The \" # Prefix to prompt the network in test mode\n",
    "\n",
    "## Load the data\n",
    "data_ = \"\"\n",
    "with open('data/dostoevski.txt', 'r', encoding=\"utf-8\") as f:\n",
    "    data_ += f.read()\n",
    "data_ = data_.lower()\n",
    "\n",
    "## Convert to 1-hot coding\n",
    "vocab = list(set(data_))\n",
    "\n",
    "data = embed_to_vocab(data_, vocab)\n",
    "\n",
    "\n",
    "in_size = out_size = len(vocab)\n",
    "lstm_size = 256 #128\n",
    "num_layers = 3\n",
    "batch_size = 64 #128\n",
    "time_steps = 100 #50\n",
    "\n",
    "NUM_TRAIN_BATCHES = 2500\n",
    "\n",
    "LEN_TEST_TEXT = 500 # Number of test characters of text to generate after training the network\n",
    "\n",
    "\n",
    "\n",
    "## Initialize the network\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "sess = tf.InteractiveSession(config=config)\n",
    "\n",
    "net = ModelNetwork(in_size = in_size,lstm_size = lstm_size,num_layers = num_layers,out_size = out_size,session = sess,learning_rate = 0.003)\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "saver = tf.train.Saver(tf.global_variables())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch:  0    loss:  4.5515    speed:  87.57701322225181  batches / s\n",
      "batch:  10    loss:  4.54942    speed:  9.669396094308187  batches / s\n",
      "batch:  20    loss:  4.54596    speed:  9.710806786616459  batches / s\n",
      "batch:  30    loss:  4.54032    speed:  8.279512670121635  batches / s\n",
      "batch:  40    loss:  4.53194    speed:  9.288436836165179  batches / s\n",
      "batch:  50    loss:  4.5188    speed:  9.021156132011201  batches / s\n",
      "batch:  60    loss:  4.5009    speed:  8.801789340810968  batches / s\n",
      "batch:  70    loss:  4.47396    speed:  9.008566947197107  batches / s\n",
      "batch:  80    loss:  4.40545    speed:  8.966771050502736  batches / s\n",
      "batch:  90    loss:  3.53479    speed:  8.901748018268357  batches / s\n",
      "batch:  100    loss:  3.3093    speed:  9.207364611444557  batches / s\n",
      "batch:  110    loss:  3.26077    speed:  9.118013125030748  batches / s\n",
      "batch:  120    loss:  3.23998    speed:  8.878695475895867  batches / s\n",
      "batch:  130    loss:  3.28134    speed:  8.230330675388222  batches / s\n",
      "batch:  140    loss:  3.21768    speed:  8.592423375745089  batches / s\n",
      "batch:  150    loss:  3.27808    speed:  9.284970305412429  batches / s\n",
      "batch:  160    loss:  3.261    speed:  8.70710899698655  batches / s\n",
      "batch:  170    loss:  3.22216    speed:  8.804126497689788  batches / s\n",
      "batch:  180    loss:  3.23945    speed:  9.084041719195543  batches / s\n",
      "batch:  190    loss:  3.25425    speed:  8.779828188662027  batches / s\n",
      "batch:  200    loss:  3.17955    speed:  9.446801268797692  batches / s\n",
      "batch:  210    loss:  3.22026    speed:  9.453106860461295  batches / s\n",
      "batch:  220    loss:  3.20151    speed:  8.637911426142056  batches / s\n",
      "batch:  230    loss:  3.21143    speed:  9.030145863316365  batches / s\n",
      "batch:  240    loss:  3.18979    speed:  9.191690402475695  batches / s\n",
      "batch:  250    loss:  3.18694    speed:  8.90919110174385  batches / s\n",
      "batch:  260    loss:  3.26254    speed:  8.80920267708102  batches / s\n",
      "batch:  270    loss:  3.18298    speed:  9.02078652374108  batches / s\n",
      "batch:  280    loss:  3.19981    speed:  8.947165399360761  batches / s\n",
      "batch:  290    loss:  3.19864    speed:  9.187882276315323  batches / s\n",
      "batch:  300    loss:  3.16426    speed:  9.138208280185308  batches / s\n",
      "batch:  310    loss:  3.14246    speed:  9.123572728768098  batches / s\n",
      "batch:  320    loss:  3.23415    speed:  8.82716485364426  batches / s\n",
      "batch:  330    loss:  3.09231    speed:  9.163806461937948  batches / s\n",
      "batch:  340    loss:  3.0485    speed:  9.01098153649646  batches / s\n",
      "batch:  350    loss:  2.95493    speed:  8.347980382991274  batches / s\n",
      "batch:  360    loss:  2.88591    speed:  9.16051794540762  batches / s\n",
      "batch:  370    loss:  2.91596    speed:  8.94590877404204  batches / s\n",
      "batch:  380    loss:  3.00122    speed:  9.354625570556788  batches / s\n",
      "batch:  390    loss:  2.89247    speed:  9.402269284160772  batches / s\n",
      "batch:  400    loss:  2.7964    speed:  9.397870920344722  batches / s\n",
      "batch:  410    loss:  2.75684    speed:  9.37841428411875  batches / s\n",
      "batch:  420    loss:  2.62449    speed:  9.396973972492745  batches / s\n",
      "batch:  430    loss:  2.64076    speed:  9.400536033773674  batches / s\n",
      "batch:  440    loss:  2.57917    speed:  9.204445701557802  batches / s\n",
      "batch:  450    loss:  2.55951    speed:  8.642809447179499  batches / s\n",
      "batch:  460    loss:  2.56383    speed:  8.95814251405909  batches / s\n",
      "batch:  470    loss:  2.46172    speed:  9.291927158875076  batches / s\n",
      "batch:  480    loss:  2.5853    speed:  8.74314898646857  batches / s\n",
      "batch:  490    loss:  2.4465    speed:  8.720916608775745  batches / s\n",
      "batch:  500    loss:  2.43583    speed:  8.53411864865413  batches / s\n",
      "batch:  510    loss:  2.42745    speed:  7.726495260450045  batches / s\n",
      "batch:  520    loss:  2.46775    speed:  8.5509611662209  batches / s\n",
      "batch:  530    loss:  2.41237    speed:  9.260844327304948  batches / s\n",
      "batch:  540    loss:  2.33922    speed:  9.123635045201137  batches / s\n",
      "batch:  550    loss:  2.35793    speed:  9.310973727841672  batches / s\n",
      "batch:  560    loss:  2.35214    speed:  9.39079411097041  batches / s\n",
      "batch:  570    loss:  2.31461    speed:  9.208639967527516  batches / s\n",
      "batch:  580    loss:  2.29913    speed:  9.362581638741082  batches / s\n",
      "batch:  590    loss:  2.28788    speed:  8.928389501264466  batches / s\n",
      "batch:  600    loss:  2.28774    speed:  8.775924475273218  batches / s\n",
      "batch:  610    loss:  2.28745    speed:  8.965545899327424  batches / s\n",
      "batch:  620    loss:  2.23522    speed:  8.890920279005432  batches / s\n",
      "batch:  630    loss:  2.18525    speed:  8.820157304957169  batches / s\n",
      "batch:  640    loss:  2.17733    speed:  8.924348225637726  batches / s\n",
      "batch:  650    loss:  2.12281    speed:  9.186632180525221  batches / s\n",
      "batch:  660    loss:  2.15335    speed:  8.923493440420541  batches / s\n",
      "batch:  670    loss:  2.07694    speed:  8.059592543145959  batches / s\n",
      "batch:  680    loss:  2.09953    speed:  8.274160800703958  batches / s\n",
      "batch:  690    loss:  2.07101    speed:  8.645443729970212  batches / s\n",
      "batch:  700    loss:  2.0769    speed:  8.722897157903837  batches / s\n",
      "batch:  710    loss:  2.0676    speed:  8.8971991275607  batches / s\n",
      "batch:  720    loss:  2.06338    speed:  8.785889033173145  batches / s\n",
      "batch:  730    loss:  2.02219    speed:  9.095264187215111  batches / s\n",
      "batch:  740    loss:  1.9739    speed:  8.611163874790515  batches / s\n",
      "batch:  750    loss:  1.95908    speed:  8.243311231369654  batches / s\n",
      "batch:  760    loss:  1.96465    speed:  9.117609573430924  batches / s\n",
      "batch:  770    loss:  1.98034    speed:  8.478413732207386  batches / s\n",
      "batch:  780    loss:  1.92367    speed:  8.990333733910768  batches / s\n",
      "batch:  790    loss:  1.93688    speed:  9.189125263499992  batches / s\n",
      "batch:  800    loss:  1.87151    speed:  8.799083115560972  batches / s\n",
      "batch:  810    loss:  1.94507    speed:  8.80768653292301  batches / s\n",
      "batch:  820    loss:  1.86587    speed:  9.445036674328811  batches / s\n",
      "batch:  830    loss:  1.86317    speed:  8.530469884793513  batches / s\n",
      "batch:  840    loss:  1.81993    speed:  9.194237827171072  batches / s\n",
      "batch:  850    loss:  1.83903    speed:  9.37842330123763  batches / s\n",
      "batch:  860    loss:  1.82421    speed:  9.235954961665968  batches / s\n",
      "batch:  870    loss:  1.75893    speed:  9.106907892344175  batches / s\n",
      "batch:  880    loss:  1.78995    speed:  9.090283728500838  batches / s\n",
      "batch:  890    loss:  1.85263    speed:  8.80458391269727  batches / s\n",
      "batch:  900    loss:  1.78639    speed:  8.625923368606292  batches / s\n",
      "batch:  910    loss:  1.76734    speed:  8.905222581200816  batches / s\n",
      "batch:  920    loss:  1.77719    speed:  9.195879299348464  batches / s\n",
      "batch:  930    loss:  1.67969    speed:  9.02769981410399  batches / s\n",
      "batch:  940    loss:  1.77127    speed:  8.633627943624232  batches / s\n",
      "batch:  950    loss:  1.72325    speed:  8.832309303331657  batches / s\n",
      "batch:  960    loss:  1.65354    speed:  8.979859745784765  batches / s\n",
      "batch:  970    loss:  1.7108    speed:  9.10024734215665  batches / s\n",
      "batch:  980    loss:  1.66374    speed:  9.275463442672004  batches / s\n",
      "batch:  990    loss:  1.65622    speed:  8.75703338307054  batches / s\n",
      "batch:  1000    loss:  1.65737    speed:  8.799127971858425  batches / s\n",
      "batch:  1010    loss:  1.69563    speed:  9.09694587516481  batches / s\n",
      "batch:  1020    loss:  1.61829    speed:  8.90672691147249  batches / s\n",
      "batch:  1030    loss:  1.70622    speed:  8.891295719575377  batches / s\n",
      "batch:  1040    loss:  1.68846    speed:  8.790656222290822  batches / s\n",
      "batch:  1050    loss:  1.62499    speed:  9.01979949798153  batches / s\n",
      "batch:  1060    loss:  1.58589    speed:  8.820116499952823  batches / s\n",
      "batch:  1070    loss:  1.61989    speed:  9.328414200678345  batches / s\n",
      "batch:  1080    loss:  1.59299    speed:  9.27042716221296  batches / s\n",
      "batch:  1090    loss:  1.65146    speed:  9.096517355575982  batches / s\n",
      "batch:  1100    loss:  1.66555    speed:  8.850225869078143  batches / s\n",
      "batch:  1110    loss:  1.60919    speed:  9.363450502447547  batches / s\n",
      "batch:  1120    loss:  1.65864    speed:  8.8184432635698  batches / s\n",
      "batch:  1130    loss:  1.58706    speed:  8.945987767877615  batches / s\n",
      "batch:  1140    loss:  1.56588    speed:  8.880473070900992  batches / s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch:  1150    loss:  1.5624    speed:  8.73667230690401  batches / s\n",
      "batch:  1160    loss:  1.59771    speed:  8.84454854604073  batches / s\n",
      "batch:  1170    loss:  1.57376    speed:  9.19462722818457  batches / s\n",
      "batch:  1180    loss:  1.55138    speed:  8.831104255230986  batches / s\n",
      "batch:  1190    loss:  1.54214    speed:  8.892512538297153  batches / s\n",
      "batch:  1200    loss:  1.53164    speed:  8.685744001715983  batches / s\n",
      "batch:  1210    loss:  1.54347    speed:  8.630468242014206  batches / s\n",
      "batch:  1220    loss:  1.57389    speed:  8.951385165936493  batches / s\n",
      "batch:  1230    loss:  1.55445    speed:  9.002095400271244  batches / s\n",
      "batch:  1240    loss:  1.51904    speed:  8.984358474511666  batches / s\n",
      "batch:  1250    loss:  1.56138    speed:  8.9742680125581  batches / s\n",
      "batch:  1260    loss:  1.54877    speed:  8.969696513474613  batches / s\n",
      "batch:  1270    loss:  1.54194    speed:  8.860036545741144  batches / s\n",
      "batch:  1280    loss:  1.55347    speed:  9.145316150659145  batches / s\n",
      "batch:  1290    loss:  1.56527    speed:  9.126077170033215  batches / s\n",
      "batch:  1300    loss:  1.52631    speed:  9.209522351146678  batches / s\n",
      "batch:  1310    loss:  1.52641    speed:  9.04702633028778  batches / s\n",
      "batch:  1320    loss:  1.5229    speed:  8.616329312041106  batches / s\n",
      "batch:  1330    loss:  1.51206    speed:  8.494160343745135  batches / s\n",
      "batch:  1340    loss:  1.54443    speed:  8.220696653740816  batches / s\n",
      "batch:  1350    loss:  1.52046    speed:  8.6130660516581  batches / s\n",
      "batch:  1360    loss:  1.51557    speed:  8.789033003821569  batches / s\n",
      "batch:  1370    loss:  1.52392    speed:  8.804531423066221  batches / s\n",
      "batch:  1380    loss:  1.49463    speed:  8.4272530227314  batches / s\n",
      "batch:  1390    loss:  1.46084    speed:  8.906824696346328  batches / s\n",
      "batch:  1400    loss:  1.4642    speed:  8.711419583692035  batches / s\n",
      "batch:  1410    loss:  1.48562    speed:  9.018770800102232  batches / s\n",
      "batch:  1420    loss:  1.52805    speed:  9.412044124252686  batches / s\n",
      "batch:  1430    loss:  1.45974    speed:  8.58317975862099  batches / s\n",
      "batch:  1440    loss:  1.51239    speed:  9.019601265319201  batches / s\n",
      "batch:  1450    loss:  1.52193    speed:  8.821693888123958  batches / s\n",
      "batch:  1460    loss:  1.48187    speed:  8.821836016184394  batches / s\n",
      "batch:  1470    loss:  1.45063    speed:  8.640534357509221  batches / s\n",
      "batch:  1480    loss:  1.46317    speed:  8.631803895678402  batches / s\n",
      "batch:  1490    loss:  1.43284    speed:  8.941830848086747  batches / s\n",
      "batch:  1500    loss:  1.44614    speed:  9.030990286338726  batches / s\n",
      "batch:  1510    loss:  1.42064    speed:  8.974951834813455  batches / s\n",
      "batch:  1520    loss:  1.49223    speed:  9.242047705120633  batches / s\n",
      "batch:  1530    loss:  1.43278    speed:  8.771978000202155  batches / s\n",
      "batch:  1540    loss:  1.47135    speed:  8.281743855619505  batches / s\n",
      "batch:  1550    loss:  1.3958    speed:  9.001832064129092  batches / s\n",
      "batch:  1560    loss:  1.41643    speed:  8.875477651880496  batches / s\n",
      "batch:  1570    loss:  1.47498    speed:  8.783380184404947  batches / s\n",
      "batch:  1580    loss:  1.4144    speed:  8.764032693197864  batches / s\n",
      "batch:  1590    loss:  1.41047    speed:  8.742407642136367  batches / s\n",
      "batch:  1600    loss:  1.36943    speed:  8.714430083506196  batches / s\n",
      "batch:  1610    loss:  1.40467    speed:  8.837993266881245  batches / s\n",
      "batch:  1620    loss:  1.42672    speed:  9.011434754915548  batches / s\n",
      "batch:  1630    loss:  1.39112    speed:  8.248876324479014  batches / s\n",
      "batch:  1640    loss:  1.40566    speed:  9.221449683148318  batches / s\n",
      "batch:  1650    loss:  1.42737    speed:  9.157095440998663  batches / s\n",
      "batch:  1660    loss:  1.44783    speed:  8.824821021026379  batches / s\n",
      "batch:  1670    loss:  1.42454    speed:  8.905232602084276  batches / s\n",
      "batch:  1680    loss:  1.40033    speed:  9.20101109018792  batches / s\n",
      "batch:  1690    loss:  1.37182    speed:  8.906211921351321  batches / s\n",
      "batch:  1700    loss:  1.4303    speed:  9.017973255283566  batches / s\n",
      "batch:  1710    loss:  1.42029    speed:  8.966047074491913  batches / s\n",
      "batch:  1720    loss:  1.4748    speed:  9.128130820965149  batches / s\n",
      "batch:  1730    loss:  1.42234    speed:  8.534020020531845  batches / s\n",
      "batch:  1740    loss:  1.37615    speed:  8.72755135913437  batches / s\n",
      "batch:  1750    loss:  1.34896    speed:  8.84792315648403  batches / s\n",
      "batch:  1760    loss:  1.33863    speed:  9.072504454214092  batches / s\n",
      "batch:  1770    loss:  1.36001    speed:  9.114542282565361  batches / s\n",
      "batch:  1780    loss:  1.36604    speed:  8.562100471926573  batches / s\n",
      "batch:  1790    loss:  1.36131    speed:  8.490509223791921  batches / s\n",
      "batch:  1800    loss:  1.3548    speed:  9.212094036143933  batches / s\n",
      "batch:  1810    loss:  1.41285    speed:  9.147863869062684  batches / s\n",
      "batch:  1820    loss:  1.36127    speed:  8.7913530712223  batches / s\n",
      "batch:  1830    loss:  1.34875    speed:  9.153834712801793  batches / s\n",
      "batch:  1840    loss:  1.3837    speed:  8.805394254575477  batches / s\n",
      "batch:  1850    loss:  1.41046    speed:  9.179854948668764  batches / s\n",
      "batch:  1860    loss:  1.35318    speed:  9.157931381541289  batches / s\n",
      "batch:  1870    loss:  1.43043    speed:  9.195954099772905  batches / s\n",
      "batch:  1880    loss:  1.35467    speed:  9.265143783880406  batches / s\n",
      "batch:  1890    loss:  1.36377    speed:  9.234258491709717  batches / s\n",
      "batch:  1900    loss:  1.38051    speed:  9.28585853849409  batches / s\n",
      "batch:  1910    loss:  1.36724    speed:  9.186629765988117  batches / s\n",
      "batch:  1920    loss:  1.38029    speed:  9.127743257673076  batches / s\n",
      "batch:  1930    loss:  1.37485    speed:  9.142850167416036  batches / s\n",
      "batch:  1940    loss:  1.3854    speed:  9.234254832254107  batches / s\n",
      "batch:  1950    loss:  1.34753    speed:  9.04456469254418  batches / s\n",
      "batch:  1960    loss:  1.40832    speed:  8.778904578025985  batches / s\n",
      "batch:  1970    loss:  1.37069    speed:  9.296242551133163  batches / s\n",
      "batch:  1980    loss:  1.30351    speed:  9.309246283584988  batches / s\n",
      "batch:  1990    loss:  1.3388    speed:  9.12437973290173  batches / s\n",
      "batch:  2000    loss:  1.38801    speed:  8.758266946869133  batches / s\n",
      "batch:  2010    loss:  1.36639    speed:  8.624932176631566  batches / s\n",
      "batch:  2020    loss:  1.29905    speed:  8.806317346175767  batches / s\n",
      "batch:  2030    loss:  1.31875    speed:  9.235072182689162  batches / s\n",
      "batch:  2040    loss:  1.3255    speed:  9.3049186063875  batches / s\n",
      "batch:  2050    loss:  1.34727    speed:  9.28500257565987  batches / s\n",
      "batch:  2060    loss:  1.33741    speed:  9.212922443051287  batches / s\n",
      "batch:  2070    loss:  1.31979    speed:  8.702864792452237  batches / s\n",
      "batch:  2080    loss:  1.32419    speed:  8.11299589946216  batches / s\n",
      "batch:  2090    loss:  1.3244    speed:  9.017148711630126  batches / s\n",
      "batch:  2100    loss:  1.29256    speed:  8.600282099154148  batches / s\n",
      "batch:  2110    loss:  1.37682    speed:  8.656160217421041  batches / s\n",
      "batch:  2120    loss:  1.32688    speed:  8.03962954906736  batches / s\n",
      "batch:  2130    loss:  1.32698    speed:  8.36875547089683  batches / s\n",
      "batch:  2140    loss:  1.35579    speed:  8.781328321457904  batches / s\n",
      "batch:  2150    loss:  1.33504    speed:  8.748984991050566  batches / s\n",
      "batch:  2160    loss:  1.3314    speed:  8.871974086067818  batches / s\n",
      "batch:  2170    loss:  1.2942    speed:  8.733664244558858  batches / s\n",
      "batch:  2180    loss:  1.31004    speed:  8.74972617365554  batches / s\n",
      "batch:  2190    loss:  1.38417    speed:  8.85540976810959  batches / s\n",
      "batch:  2200    loss:  1.36416    speed:  8.874311116467378  batches / s\n",
      "batch:  2210    loss:  1.31361    speed:  8.77638392277141  batches / s\n",
      "batch:  2220    loss:  1.34407    speed:  8.54037733759086  batches / s\n",
      "batch:  2230    loss:  1.27412    speed:  8.792854933272645  batches / s\n",
      "batch:  2240    loss:  1.29943    speed:  8.45116146891415  batches / s\n",
      "batch:  2250    loss:  1.29986    speed:  8.550859010570962  batches / s\n",
      "batch:  2260    loss:  1.36471    speed:  8.317664642028625  batches / s\n",
      "batch:  2270    loss:  1.33598    speed:  7.5638668063169545  batches / s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch:  2280    loss:  1.31761    speed:  8.254337173499831  batches / s\n",
      "batch:  2290    loss:  1.25886    speed:  8.746241645979259  batches / s\n",
      "batch:  2300    loss:  1.28515    speed:  8.310052409766287  batches / s\n",
      "batch:  2310    loss:  1.26754    speed:  8.716034554057734  batches / s\n",
      "batch:  2320    loss:  1.306    speed:  9.060559379215642  batches / s\n",
      "batch:  2330    loss:  1.29526    speed:  8.736781862388995  batches / s\n",
      "batch:  2340    loss:  1.3165    speed:  9.156245061601847  batches / s\n",
      "batch:  2350    loss:  1.27769    speed:  9.101083599241544  batches / s\n",
      "batch:  2360    loss:  1.29796    speed:  8.970915700058534  batches / s\n",
      "batch:  2370    loss:  1.30569    speed:  9.244544064538022  batches / s\n",
      "batch:  2380    loss:  1.26502    speed:  9.118552703164685  batches / s\n",
      "batch:  2390    loss:  1.27709    speed:  8.92514562849793  batches / s\n",
      "batch:  2400    loss:  1.29128    speed:  8.546053655709875  batches / s\n",
      "batch:  2410    loss:  1.29486    speed:  8.536966476740732  batches / s\n",
      "batch:  2420    loss:  1.24473    speed:  9.058085873198278  batches / s\n",
      "batch:  2430    loss:  1.28597    speed:  8.433813917301851  batches / s\n",
      "batch:  2440    loss:  1.31902    speed:  8.9507246040317  batches / s\n",
      "batch:  2450    loss:  1.29213    speed:  8.164543056307341  batches / s\n",
      "batch:  2460    loss:  1.34103    speed:  8.786723180293425  batches / s\n",
      "batch:  2470    loss:  1.28933    speed:  8.671434645245874  batches / s\n",
      "batch:  2480    loss:  1.24874    speed:  7.810348102691297  batches / s\n",
      "batch:  2490    loss:  1.33784    speed:  8.565528826602906  batches / s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'./model.ckpt'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if ckpt_file == \"\":\n",
    "    last_time = time.time()\n",
    "\n",
    "    batch = np.zeros((batch_size, time_steps, in_size))\n",
    "    batch_y = np.zeros((batch_size, time_steps, in_size))\n",
    "\n",
    "    possible_batch_ids = range(data.shape[0]-time_steps-1)\n",
    "    for i in range(NUM_TRAIN_BATCHES):\n",
    "        # Sample time_steps consecutive samples from the dataset text file\n",
    "        batch_id = random.sample( possible_batch_ids, batch_size )\n",
    "\n",
    "        for j in range(time_steps):\n",
    "            ind1 = [k+j for k in batch_id]\n",
    "            ind2 = [k+j+1 for k in batch_id]\n",
    "\n",
    "            batch[:, j, :] = data[ind1, :]\n",
    "            batch_y[:, j, :] = data[ind2, :]\n",
    "\n",
    "\n",
    "        cst = net.train_batch(batch, batch_y)\n",
    "\n",
    "        if (i%10) == 0:\n",
    "            new_time = time.time()\n",
    "            diff = new_time - last_time\n",
    "            last_time = new_time\n",
    "\n",
    "            print(\"batch: \",i,\"   loss: \",cst,\"   speed: \",(100.0/diff),\" batches / s\")\n",
    "\n",
    "saver.save(sess, \"./model.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "generate text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SENTENCE:\n",
      "город просыпался распистроской.\n",
      "\n",
      "— так! подаль, а тут вот уж и аршился на дорогу! ну, болезнь!!.. хотя не выдержить, — сказал разумихин, — придавила пугу противоречило скажет, лицо его осоветия ее, присужден а родя…\n",
      "\n",
      "— вздор! — как бы вылоси, — вскочила дуня, — ох, познаний, что «ною, господино ставшись, дуня попросила соня.\n",
      "\n",
      "неразрыми ей было красоты, после передолжиться говорющими-то другому листое положения.\n",
      "\n",
      "— метяко, родион романыч.) — крикнул разумихин, — вы, авдотья романовна, и, родя.\n",
      "\n",
      "— ну так разумихин\n"
     ]
    }
   ],
   "source": [
    "TEST_PREFIX = \"город просыпался \"\n",
    "\n",
    "for i in range(len(TEST_PREFIX)):\n",
    "    out = net.run_step( embed_to_vocab(TEST_PREFIX[i], vocab) , i==0)\n",
    "\n",
    "print(\"SENTENCE:\")\n",
    "gen_str = TEST_PREFIX\n",
    "for i in range(LEN_TEST_TEXT):\n",
    "    element = np.random.choice( range(len(vocab)), p=out ) # Sample character from the network according to the generated output probabilities\n",
    "    gen_str += vocab[element]\n",
    "\n",
    "    out = net.run_step( embed_to_vocab(vocab[element], vocab) , False )\n",
    "print(gen_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
